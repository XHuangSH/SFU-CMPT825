{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "### Task\n",
    "The syntax of a natural language, similar to the syntax of a programming language involves the arrangement of tokens into meaningful groups. Phrasal chunking is the task of finding non-recursive syntactic groups of words. \n",
    "\n",
    "In this task we are given a clean training set, and a dirty test set in which noise is added into the text. We are supposed to implement a character level method which creates 2 seperate one hot, and one multi hot, vector for each word.\n",
    "\n",
    "The 3 vectors are:\n",
    "\n",
    "First Char (One Hot)\n",
    "\n",
    "Middle (Multi Hot)\n",
    "\n",
    "Last (One Hot)\n",
    "\n",
    "### Method\n",
    "\n",
    "These vectors are to be concatenated to the word embedding from the text, then feed through an LSTM. At each time step the LSTM will output a tag for that word.\n",
    "\n",
    "As an alternative to this weird one hot representing, we can make an additional character embedding matrix for the one / multi hot vectors and learn this during training time. This can be done with a basic matrix multiplication. Both methods were implemented, and the character embedding matrix gave us better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chunker import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the default solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1027/1027 [00:01<00:00, 822.65it/s]\n"
     ]
    }
   ],
   "source": [
    "chunker = LSTMTagger(os.path.join('../data', 'train.txt.gz'), os.path.join('../data', 'chunker'), '.tar')\n",
    "decoder_output = chunker.decode('../data/input/dev.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 23663 tokens with 11896 phrases; found: 11672 phrases; correct: 8568.\n",
      "accuracy:  84.35%; (non-O)\n",
      "accuracy:  85.65%; precision:  73.41%; recall:  72.02%; FB1:  72.71\n",
      "             ADJP: precision:  36.49%; recall:  11.95%; FB1:  18.00  74\n",
      "             ADVP: precision:  71.36%; recall:  39.45%; FB1:  50.81  220\n",
      "            CONJP: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "             INTJ: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "               NP: precision:  70.33%; recall:  76.80%; FB1:  73.42  6811\n",
      "               PP: precision:  92.40%; recall:  87.14%; FB1:  89.69  2302\n",
      "              PRT: precision:  65.00%; recall:  57.78%; FB1:  61.18  40\n",
      "             SBAR: precision:  84.62%; recall:  41.77%; FB1:  55.93  117\n",
      "               VP: precision:  63.66%; recall:  58.25%; FB1:  60.83  2108\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(73.40644276901988, 72.02420981842637, 72.70875763747455)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_output = [ output for sent in decoder_output for output in sent ]\n",
    "import conlleval\n",
    "true_seqs = []\n",
    "with open(os.path.join('../data','reference','dev.out')) as r:\n",
    "    for sent in conlleval.read_file(r):\n",
    "        true_seqs += sent.split()\n",
    "conlleval.evaluate(true_seqs, flat_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at given code to figure out what's going on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need this function to read for next few functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, gzip\n",
    "import re\n",
    "\n",
    "def read_conll(handle, input_idx=0, label_idx=2):\n",
    "    conll_data = []\n",
    "    contents = re.sub(r'\\n\\s*\\n', r'\\n\\n', handle.read())\n",
    "    contents = contents.rstrip()\n",
    "    for sent_string in contents.split('\\n\\n'):\n",
    "        annotations = list(zip(*[ word_string.split() for word_string in sent_string.split('\\n') ]))\n",
    "        assert(input_idx < len(annotations))\n",
    "        if label_idx < 0:\n",
    "            conll_data.append( annotations[input_idx] )\n",
    "            logging.info(\"CoNLL: {}\".format( \" \".join(annotations[input_idx])))\n",
    "        else:\n",
    "            assert(label_idx < len(annotations))\n",
    "            conll_data.append(( annotations[input_idx], annotations[label_idx] ))\n",
    "            logging.info(\"CoNLL: {} ||| {}\".format( \" \".join(annotations[input_idx]), \" \".join(annotations[label_idx])))\n",
    "    return conll_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to determine the number of unique characters\n",
    "\n",
    "In class Anoop seemed to suggest it is 100 (this changed in the instructions later to just using the python symbols, too lazy to change now)\n",
    "\n",
    "Just loop through and append each char in a dict to count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n",
      "['!', '#', '$', '%', '&', \"'\", '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "{'!': 0, '#': 1, '$': 2, '%': 3, '&': 4, \"'\": 5, '*': 6, ',': 7, '-': 8, '.': 9, '/': 10, '0': 11, '1': 12, '2': 13, '3': 14, '4': 15, '5': 16, '6': 17, '7': 18, '8': 19, '9': 20, ':': 21, ';': 22, '=': 23, '?': 24, 'A': 25, 'B': 26, 'C': 27, 'D': 28, 'E': 29, 'F': 30, 'G': 31, 'H': 32, 'I': 33, 'J': 34, 'K': 35, 'L': 36, 'M': 37, 'N': 38, 'O': 39, 'P': 40, 'Q': 41, 'R': 42, 'S': 43, 'T': 44, 'U': 45, 'V': 46, 'W': 47, 'X': 48, 'Y': 49, 'Z': 50, '[': 51, '\\\\': 52, ']': 53, '`': 54, 'a': 55, 'b': 56, 'c': 57, 'd': 58, 'e': 59, 'f': 60, 'g': 61, 'h': 62, 'i': 63, 'j': 64, 'k': 65, 'l': 66, 'm': 67, 'n': 68, 'o': 69, 'p': 70, 'q': 71, 'r': 72, 's': 73, 't': 74, 'u': 75, 'v': 76, 'w': 77, 'x': 78, 'y': 79, 'z': 80}\n"
     ]
    }
   ],
   "source": [
    "trainfile = '../data/train.txt.gz'\n",
    "\n",
    "chars = set()\n",
    "with gzip.open(trainfile, 'rt') as f:\n",
    "    contents = re.sub(r'\\n\\s*\\n', r'\\n\\n', f.read())\n",
    "    contents = contents.rstrip()\n",
    "    for sent_string in contents.split('\\n\\n'):\n",
    "        # Tuple of (words, labels)\n",
    "        annotations = list(zip(*[ word_string.split() for word_string in sent_string.split('\\n') ]))\n",
    "        words = annotations[0]\n",
    "        for word in words:\n",
    "            for char in word:\n",
    "                chars.add(char)\n",
    "\n",
    "chars = list(chars)\n",
    "chars = sorted(chars)\n",
    "print(len(chars))\n",
    "print(chars)\n",
    "charToDex = {char: dex for dex, char in enumerate(chars)}\n",
    "\n",
    "print(charToDex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems short but also complete? Where is he getting these extra 19 characters from? \n",
    "\n",
    "Turns out he is using the python built in string \n",
    "\n",
    "Using a single random for unknown VS 19 random for unknown is likely better anyways\n",
    "\n",
    "I don't know of any work that uses 19 unknowns???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can just make 3 seperate one hot vectors for each word\n",
    "\n",
    "Multiply them by the character embedding matrix then use a torch view to rearrange\n",
    "\n",
    "So lets say we have 3 words, we have 9 x 81, multiply by embedding matrix, we get 9 x embedding size use torch.view(dim[0]/3, 3, embDim).sum(1) and we have our embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[UNK]', 'in', 'the', 'pound', 'is', 'widely', 'expected', 'to', 'take', 'another', 'sharp', 'dive', 'if', 'trade', 'figures', 'for', 'September', ',', 'due', 'for', 'release', 'tomorrow', ',', 'fail', 'to', 'show', 'a', 'substantial', 'improvement', 'from', 'July', 'and', 'August', \"'s\", 'near-record', '[UNK]', '.'), ('NN', 'IN', 'DT', 'NN', 'VBZ', 'RB', 'VBN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NNS', 'IN', 'NNP', ',', 'JJ', 'IN', 'NN', 'NN', ',', 'VB', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'CC', 'NNP', 'POS', 'JJ', 'NNS', '.'), ('B-NP', 'B-PP', 'B-NP', 'I-NP', 'B-VP', 'I-VP', 'I-VP', 'I-VP', 'I-VP', 'B-NP', 'I-NP', 'I-NP', 'B-SBAR', 'B-NP', 'I-NP', 'B-PP', 'B-NP', 'O', 'B-ADJP', 'B-PP', 'B-NP', 'B-NP', 'O', 'B-VP', 'I-VP', 'I-VP', 'B-NP', 'I-NP', 'I-NP', 'B-PP', 'B-NP', 'I-NP', 'I-NP', 'B-NP', 'I-NP', 'I-NP', 'O')]\n",
      "pound\n",
      "Word: pound First: p Middle: oun Last: d\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "trainfile = '../data/train.txt.gz'\n",
    "\n",
    "with gzip.open(trainfile, 'rt') as f:\n",
    "    contents = re.sub(r'\\n\\s*\\n', r'\\n\\n', f.read())\n",
    "    contents = contents.rstrip()\n",
    "    for sent_string in contents.split('\\n\\n'):\n",
    "        # Tuple of (words, labels)\n",
    "        annotations = list(zip(*[ word_string.split() for word_string in sent_string.split('\\n') ]))\n",
    "        \n",
    "        print(annotations)\n",
    "        print(annotations[0][3])\n",
    "        word = annotations[0][3]\n",
    "        \n",
    "        oneHot = np.zeros((3, len(charToDex)))\n",
    "        \n",
    "        if len(word) >= 3:    \n",
    "            first = word[0]\n",
    "            last = word[-1]\n",
    "            mid = word[1:-1]\n",
    "            print(\"Word: {} First: {} Middle: {} Last: {}\".format(word, first, mid, last))\n",
    "\n",
    "            # Could be more than one so just add\n",
    "            for c in mid:\n",
    "                oneHot[1, charToDex[c]] += 1.0\n",
    "                \n",
    "            oneHot[2, charToDex[last]] += 1.0\n",
    "        elif len(word) == 2:\n",
    "            first = word[0]\n",
    "            last = word[-1]\n",
    "            oneHot[2, charToDex[last]] += 1.0\n",
    "        else:\n",
    "            first = word[0]\n",
    "            \n",
    "        oneHot[0, charToDex[first]] += 1.0\n",
    "        \n",
    "        print(oneHot)\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above looks good to me. Let's make this into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll_char(handle, input_idx=0, label_idx=2):\n",
    "    conll_data = []\n",
    "    contents = re.sub(r'\\n\\s*\\n', r'\\n\\n', handle.read())\n",
    "    contents = contents.rstrip()\n",
    "    for sent_string in contents.split('\\n\\n'):\n",
    "        annotations = list(zip(*[ word_string.split() for word_string in sent_string.split('\\n') ]))\n",
    "        assert(input_idx < len(annotations))\n",
    "        if label_idx < 0:\n",
    "            conll_data.append( annotations[input_idx] )\n",
    "            logging.info(\"CoNLL: {}\".format( \" \".join(annotations[input_idx])))\n",
    "        else:\n",
    "            assert(label_idx < len(annotations))\n",
    "            \n",
    "            charTups = []\n",
    "            for word in annotations[input_idx]:\n",
    "        \n",
    "                first = mid = last = None\n",
    "                refLen = len(word)\n",
    "                if refLen >= 3:    \n",
    "                    first = word[0]\n",
    "                    last = word[-1]\n",
    "                    mid = word[1:-1]\n",
    "                elif refLen == 2:\n",
    "                    first = word[0]\n",
    "                    last = word[-1]\n",
    "                else:\n",
    "                    first = word[0]\n",
    "\n",
    "                charTups.append( (first, mid, last, refLen) )\n",
    "                \n",
    "            conll_data.append( ( annotations[input_idx], annotations[label_idx] , charTups) )\n",
    "            logging.info(\"CoNLL: {} ||| {}\".format( \" \".join(annotations[input_idx]), \" \".join(annotations[label_idx])))\n",
    "            \n",
    "    return conll_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8936\n",
      "('[UNK]', 'in', 'the', 'pound', 'is', 'widely', 'expected', 'to', 'take', 'another', 'sharp', 'dive', 'if', 'trade', 'figures', 'for', 'September', ',', 'due', 'for', 'release', 'tomorrow', ',', 'fail', 'to', 'show', 'a', 'substantial', 'improvement', 'from', 'July', 'and', 'August', \"'s\", 'near-record', '[UNK]', '.')\n",
      "\n",
      "[('[', 'UNK', ']', 5), ('i', None, 'n', 2), ('t', 'h', 'e', 3), ('p', 'oun', 'd', 5), ('i', None, 's', 2), ('w', 'idel', 'y', 6), ('e', 'xpecte', 'd', 8), ('t', None, 'o', 2), ('t', 'ak', 'e', 4), ('a', 'nothe', 'r', 7), ('s', 'har', 'p', 5), ('d', 'iv', 'e', 4), ('i', None, 'f', 2), ('t', 'rad', 'e', 5), ('f', 'igure', 's', 7), ('f', 'o', 'r', 3), ('S', 'eptembe', 'r', 9), (',', None, None, 1), ('d', 'u', 'e', 3), ('f', 'o', 'r', 3), ('r', 'eleas', 'e', 7), ('t', 'omorro', 'w', 8), (',', None, None, 1), ('f', 'ai', 'l', 4), ('t', None, 'o', 2), ('s', 'ho', 'w', 4), ('a', None, None, 1), ('s', 'ubstantia', 'l', 11), ('i', 'mprovemen', 't', 11), ('f', 'ro', 'm', 4), ('J', 'ul', 'y', 4), ('a', 'n', 'd', 3), ('A', 'ugus', 't', 6), (\"'\", None, 's', 2), ('n', 'ear-recor', 'd', 11), ('[', 'UNK', ']', 5), ('.', None, None, 1)]\n"
     ]
    }
   ],
   "source": [
    "with gzip.open(trainfile, 'rt') as f:\n",
    "    conll_w_char = read_conll_char(f)\n",
    "    \n",
    "print(len(conll_w_char))\n",
    "print(conll_w_char[0][0])\n",
    "print()\n",
    "print(conll_w_char[0][2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to create charToDex with annotation format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On [0/8936]\n",
      "On [500/8936]\n",
      "On [1000/8936]\n",
      "On [1500/8936]\n",
      "On [2000/8936]\n",
      "On [2500/8936]\n",
      "On [3000/8936]\n",
      "On [3500/8936]\n",
      "On [4000/8936]\n",
      "On [4500/8936]\n",
      "On [5000/8936]\n",
      "On [5500/8936]\n",
      "On [6000/8936]\n",
      "On [6500/8936]\n",
      "On [7000/8936]\n",
      "On [7500/8936]\n",
      "On [8000/8936]\n",
      "On [8500/8936]\n",
      "81\n",
      "['!', '#', '$', '%', '&', \"'\", '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "{'!': 0, '#': 1, '$': 2, '%': 3, '&': 4, \"'\": 5, '*': 6, ',': 7, '-': 8, '.': 9, '/': 10, '0': 11, '1': 12, '2': 13, '3': 14, '4': 15, '5': 16, '6': 17, '7': 18, '8': 19, '9': 20, ':': 21, ';': 22, '=': 23, '?': 24, 'A': 25, 'B': 26, 'C': 27, 'D': 28, 'E': 29, 'F': 30, 'G': 31, 'H': 32, 'I': 33, 'J': 34, 'K': 35, 'L': 36, 'M': 37, 'N': 38, 'O': 39, 'P': 40, 'Q': 41, 'R': 42, 'S': 43, 'T': 44, 'U': 45, 'V': 46, 'W': 47, 'X': 48, 'Y': 49, 'Z': 50, '[': 51, '\\\\': 52, ']': 53, '`': 54, 'a': 55, 'b': 56, 'c': 57, 'd': 58, 'e': 59, 'f': 60, 'g': 61, 'h': 62, 'i': 63, 'j': 64, 'k': 65, 'l': 66, 'm': 67, 'n': 68, 'o': 69, 'p': 70, 'q': 71, 'r': 72, 's': 73, 't': 74, 'u': 75, 'v': 76, 'w': 77, 'x': 78, 'y': 79, 'z': 80}\n"
     ]
    }
   ],
   "source": [
    "# Need to create charToDex with annotation format\n",
    "chars = set()\n",
    "for annDex, ann in enumerate(conll_w_char):\n",
    "    if (annDex % 500) == 0:\n",
    "        print(\"On [{}/{}]\".format(annDex, len(conll_w_char)))\n",
    "    # Tuple of (words, labels, charTups)\n",
    "    charTups = ann[-1]\n",
    "    #print(charTups)\n",
    "    \n",
    "    for ct in charTups:\n",
    "        # Don't need length\n",
    "        chars.add(ct[0])\n",
    "        \n",
    "        if ct[2] is not None:\n",
    "            chars.add(ct[2])\n",
    "        \n",
    "        if ct[1] is not None:\n",
    "            for char in ct[1]:\n",
    "                chars.add(char)\n",
    "                \n",
    "chars = list(chars)\n",
    "chars = sorted(chars)\n",
    "print(len(chars))\n",
    "print(chars)\n",
    "charToDex = {char: dex for dex, char in enumerate(chars)}\n",
    "# Seems short but also complete? Where is he getting these extra 19 characters from?\n",
    "print(charToDex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Might as well try to integrate with the other code as much as possible "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n",
      "['!', '#', '$', '%', '&', \"'\", '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "{'!': 0, '#': 1, '$': 2, '%': 3, '&': 4, \"'\": 5, '*': 6, ',': 7, '-': 8, '.': 9, '/': 10, '0': 11, '1': 12, '2': 13, '3': 14, '4': 15, '5': 16, '6': 17, '7': 18, '8': 19, '9': 20, ':': 21, ';': 22, '=': 23, '?': 24, 'A': 25, 'B': 26, 'C': 27, 'D': 28, 'E': 29, 'F': 30, 'G': 31, 'H': 32, 'I': 33, 'J': 34, 'K': 35, 'L': 36, 'M': 37, 'N': 38, 'O': 39, 'P': 40, 'Q': 41, 'R': 42, 'S': 43, 'T': 44, 'U': 45, 'V': 46, 'W': 47, 'X': 48, 'Y': 49, 'Z': 50, '[': 51, '\\\\': 52, ']': 53, '`': 54, 'a': 55, 'b': 56, 'c': 57, 'd': 58, 'e': 59, 'f': 60, 'g': 61, 'h': 62, 'i': 63, 'j': 64, 'k': 65, 'l': 66, 'm': 67, 'n': 68, 'o': 69, 'p': 70, 'q': 71, 'r': 72, 's': 73, 't': 74, 'u': 75, 'v': 76, 'w': 77, 'x': 78, 'y': 79, 'z': 80}\n"
     ]
    }
   ],
   "source": [
    "# Might as well try to integrate with the other code as much as possible \n",
    "word_to_ix = {}\n",
    "tag_to_ix = {}\n",
    "ix_to_tag = []\n",
    "chars = set()\n",
    "\n",
    "for sent, tags, charTups in conll_w_char:\n",
    "    \n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "            \n",
    "    for tag in tags:\n",
    "        if tag not in tag_to_ix:\n",
    "            tag_to_ix[tag] = len(tag_to_ix)\n",
    "            ix_to_tag.append(tag)\n",
    "\n",
    "    for ct in charTups:\n",
    "        # Don't need length\n",
    "        chars.add(ct[0])\n",
    "        \n",
    "        if ct[2] is not None:\n",
    "            chars.add(ct[2])\n",
    "        \n",
    "        if ct[1] is not None:\n",
    "            for char in ct[1]:\n",
    "                chars.add(char)\n",
    "                \n",
    "chars = list(chars)\n",
    "chars = sorted(chars)\n",
    "print(len(chars))\n",
    "print(chars)\n",
    "charToDex = {char: dex for dex, char in enumerate(chars)}\n",
    "# Seems short but also complete? Where is he getting these extra 19 characters from?\n",
    "print(charToDex)\n",
    "charToDex['unk'] = len(charToDex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix, unk):\n",
    "\n",
    "    if type(seq[0]) == tuple:\n",
    "        charOHs = []\n",
    "        for charTup in seq:\n",
    "            oneHot = np.zeros((3, len(to_ix)))\n",
    "            # Last item in tuple was saved as word len\n",
    "            refLen = charTup[-1]\n",
    "            if refLen >= 3:    \n",
    "                first = charTup[0]\n",
    "                mid = charTup[1]\n",
    "                last = charTup[2] \n",
    "                \n",
    "                # Could be more than one so just add\n",
    "                for c in mid:\n",
    "                    if c not in charToDex:\n",
    "                        c = \"unk\"\n",
    "                    oneHot[1, charToDex[c]] += 1.0\n",
    "\n",
    "                if last not in charToDex:\n",
    "                    last = \"unk\"\n",
    "                oneHot[2, charToDex[last]] += 1.0\n",
    "            elif refLen == 2:\n",
    "                first = charTup[0]\n",
    "                last = charTup[2]\n",
    "                \n",
    "                if last not in charToDex:\n",
    "                    last = \"unk\"\n",
    "                    \n",
    "                oneHot[2, charToDex[last]] += 1.0\n",
    "            else:\n",
    "                first = charTup[0]\n",
    "            \n",
    "            if first not in charToDex:\n",
    "                first = \"unk\"\n",
    "                \n",
    "            oneHot[0, charToDex[first]] += 1.0\n",
    "            \n",
    "            charOHs.append(oneHot)\n",
    "        charOHs = np.stack(charOHs)\n",
    "        return torch.from_numpy(charOHs).type(torch.FloatTensor)\n",
    "    else:\n",
    "        idxs = []\n",
    "        if unk not in to_ix:\n",
    "            idxs = [to_ix[w] for w in seq]\n",
    "        else:\n",
    "            idxs = [to_ix[w] for w in map(lambda w: unk if w not in to_ix else w, seq)]\n",
    "        return torch.tensor(idxs, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([37, 3, 82])\n",
      "torch.Size([111, 82])\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "charSparse = prepare_sequence(conll_w_char[0][2], charToDex, unk=None)\n",
    "print(charSparse.shape)\n",
    "print(charSparse.view(-1,charSparse.shape[-1]).shape)\n",
    "print(len(conll_w_char[0][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the original code now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from original code by Robert Guthrie\n",
    "\n",
    "import numpy as np\n",
    "import os, sys, optparse, gzip, re, logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "\n",
    "def read_conll(handle, input_idx=0, label_idx=2):\n",
    "    conll_data = []\n",
    "    contents = re.sub(r'\\n\\s*\\n', r'\\n\\n', handle.read())\n",
    "    contents = contents.rstrip()\n",
    "    for sent_string in contents.split('\\n\\n'):\n",
    "        annotations = list(zip(*[ word_string.split() for word_string in sent_string.split('\\n') ]))\n",
    "        assert(input_idx < len(annotations))\n",
    "        if label_idx < 0:\n",
    "            \n",
    "            charTups = []\n",
    "            for word in annotations[input_idx]:\n",
    "        \n",
    "                first = mid = last = None\n",
    "                refLen = len(word)\n",
    "                if refLen >= 3:    \n",
    "                    first = word[0]\n",
    "                    last = word[-1]\n",
    "                    mid = word[1:-1]\n",
    "                elif refLen == 2:\n",
    "                    first = word[0]\n",
    "                    last = word[-1]\n",
    "                else:\n",
    "                    first = word[0]\n",
    "\n",
    "                charTups.append( (first, mid, last, refLen) )\n",
    "                \n",
    "            conll_data.append( ( annotations[input_idx], charTups) )\n",
    "            \n",
    "            #conll_data.append( annotations[input_idx] )\n",
    "            logging.info(\"CoNLL: {}\".format( \" \".join(annotations[input_idx])))\n",
    "        else:\n",
    "            assert(label_idx < len(annotations))\n",
    "            \n",
    "            charTups = []\n",
    "            for word in annotations[input_idx]:\n",
    "        \n",
    "                first = mid = last = None\n",
    "                refLen = len(word)\n",
    "                if refLen >= 3:    \n",
    "                    first = word[0]\n",
    "                    last = word[-1]\n",
    "                    mid = word[1:-1]\n",
    "                elif refLen == 2:\n",
    "                    first = word[0]\n",
    "                    last = word[-1]\n",
    "                else:\n",
    "                    first = word[0]\n",
    "\n",
    "                charTups.append( (first, mid, last, refLen) )\n",
    "                \n",
    "            conll_data.append( ( annotations[input_idx], annotations[label_idx] , charTups) )\n",
    "            logging.info(\"CoNLL: {} ||| {}\".format( \" \".join(annotations[input_idx]), \" \".join(annotations[label_idx])))\n",
    "            \n",
    "    return conll_data\n",
    "\n",
    "def prepare_sequence(seq, to_ix, unk):\n",
    "\n",
    "    if type(seq[0]) == tuple:\n",
    "        charOHs = []\n",
    "        for charTup in seq:\n",
    "            oneHot = np.zeros((3, len(to_ix)))\n",
    "            # Last item in tuple was saved as word len\n",
    "            refLen = charTup[-1]\n",
    "            if refLen >= 3:    \n",
    "                first = charTup[0]\n",
    "                mid = charTup[1]\n",
    "                last = charTup[2] \n",
    "                \n",
    "                # Could be more than one so just add\n",
    "                for c in mid:\n",
    "                    if c not in to_ix:\n",
    "                        c = \"unk\"\n",
    "                    oneHot[1, to_ix[c]] += 1.0\n",
    "\n",
    "                if last not in to_ix:\n",
    "                    last = \"unk\"\n",
    "                oneHot[2, to_ix[last]] += 1.0\n",
    "            elif refLen == 2:\n",
    "                first = charTup[0]\n",
    "                last = charTup[2]\n",
    "                \n",
    "                if last not in to_ix:\n",
    "                    last = \"unk\"\n",
    "                    \n",
    "                oneHot[2, to_ix[last]] += 1.0\n",
    "            else:\n",
    "                first = charTup[0]\n",
    "            \n",
    "            if first not in to_ix:\n",
    "                first = \"unk\"\n",
    "                \n",
    "            oneHot[0, to_ix[first]] += 1.0\n",
    "            \n",
    "            charOHs.append(oneHot)\n",
    "        charOHs = np.stack(charOHs)\n",
    "        charOHs = torch.from_numpy(charOHs).type(torch.FloatTensor)\n",
    "        charOHs = charOHs.view(-1, charOHs.shape[-1])\n",
    "        return charOHs\n",
    "    else:\n",
    "        idxs = []\n",
    "        if unk not in to_ix:\n",
    "            idxs = [to_ix[w] for w in seq]\n",
    "        else:\n",
    "            idxs = [to_ix[w] for w in map(lambda w: unk if w not in to_ix else w, seq)]\n",
    "        return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to implement weird baseline model which appends just the one hot char vects for some reason???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTaggerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, char_size):\n",
    "        torch.manual_seed(1)\n",
    "        super(LSTMTaggerModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.char_embeddings = nn.Parameter(torch.zeros(char_size, embedding_dim))\n",
    "        \n",
    "        torch.nn.init.normal_(self.char_embeddings)\n",
    "        \n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        # If sum chars\n",
    "        # self.lstm = nn.LSTM(embedding_dim * 2, hidden_dim, bidirectional=False)\n",
    "        # If 4 unique embeds\n",
    "        self.lstm = nn.LSTM(embedding_dim + 82 * 3, hidden_dim, bidirectional=False)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence, charEmbed):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        \n",
    "        charEmbed = charEmbed.view(int(charEmbed.shape[0] / 3), 3, charEmbed.shape[1])\n",
    "        charEmbed = charEmbed.view(charEmbed.shape[0], -1)\n",
    "        \n",
    "        embeds = torch.cat([embeds, charEmbed],-1)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "\n",
    "class LSTMTagger:\n",
    "\n",
    "    def __init__(self, trainfile, modelfile, modelsuffix, unk=\"[UNK]\", epochs=10, embedding_dim=128, hidden_dim=64):\n",
    "        self.unk = unk\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.epochs = epochs\n",
    "        self.modelfile = modelfile\n",
    "        self.modelsuffix = modelsuffix\n",
    "        self.training_data = []\n",
    "        if trainfile[-3:] == '.gz':\n",
    "            with gzip.open(trainfile, 'rt') as f:\n",
    "                self.training_data = read_conll(f)\n",
    "        else:\n",
    "            with open(trainfile, 'r') as f:\n",
    "                self.training_data = read_conll(f)\n",
    "\n",
    "        self.word_to_ix = {} # replaces words with an index (one-hot vector)\n",
    "        self.tag_to_ix = {} # replace output labels / tags with an index\n",
    "        self.ix_to_tag = [] # during inference we produce tag indices so we have to map it back to a tag\n",
    "\n",
    "        chars = set()\n",
    "        for sent, tags, charTups in self.training_data:\n",
    "            \n",
    "            for word in sent:\n",
    "                if word not in self.word_to_ix:\n",
    "                    self.word_to_ix[word] = len(self.word_to_ix)\n",
    "                    \n",
    "            for tag in tags:\n",
    "                if tag not in self.tag_to_ix:\n",
    "                    self.tag_to_ix[tag] = len(self.tag_to_ix)\n",
    "                    self.ix_to_tag.append(tag)\n",
    "                    \n",
    "            for ct in charTups:\n",
    "    \n",
    "                chars.add(ct[0])\n",
    "\n",
    "                if ct[2] is not None:\n",
    "                    chars.add(ct[2])\n",
    "\n",
    "                if ct[1] is not None:\n",
    "                    for char in ct[1]:\n",
    "                        chars.add(char)\n",
    "\n",
    "        chars = list(chars)\n",
    "        chars = sorted(chars)\n",
    "        charToDex = {char: dex for dex, char in enumerate(chars)}\n",
    "        charToDex['unk'] = len(charToDex)\n",
    "        self.charToDex = charToDex\n",
    "        \n",
    "        logging.info(\"word_to_ix:\", self.word_to_ix)\n",
    "        logging.info(\"tag_to_ix:\", self.tag_to_ix)\n",
    "        logging.info(\"ix_to_tag:\", self.ix_to_tag)\n",
    "        logging.info(\"char_to_dex:\", self.charToDex)\n",
    "        \n",
    "        print(\"Creating Modified Model\")\n",
    "        self.model = LSTMTaggerModel(self.embedding_dim, self.hidden_dim, len(self.word_to_ix), len(self.tag_to_ix), len(self.charToDex))\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=0.01)\n",
    "\n",
    "    def argmax(self, seq, charTups):\n",
    "        output = []\n",
    "        with torch.no_grad():\n",
    "            inputs = prepare_sequence(seq, self.word_to_ix, self.unk)\n",
    "            charEmbeds = prepare_sequence(charTups, self.charToDex, \"unk\")\n",
    "            tag_scores = self.model(inputs, charEmbeds)\n",
    "            for i in range(len(inputs)):\n",
    "                output.append(self.ix_to_tag[int(tag_scores[i].argmax(dim=0))])\n",
    "        return output\n",
    "\n",
    "    def train(self):\n",
    "        loss_function = nn.NLLLoss()\n",
    "\n",
    "        self.model.train()\n",
    "        loss = float(\"inf\")\n",
    "        for epoch in range(self.epochs):\n",
    "            for sentence, tags, charTups in tqdm.tqdm(self.training_data):\n",
    "                # Step 1. Remember that Pytorch accumulates gradients.\n",
    "                # We need to clear them out before each instance\n",
    "                self.model.zero_grad()\n",
    "\n",
    "                # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "                # Tensors of word indices.\n",
    "                sentence_in = prepare_sequence(sentence, self.word_to_ix, self.unk)\n",
    "                targets = prepare_sequence(tags, self.tag_to_ix, self.unk)\n",
    "                charEmbeds = prepare_sequence(charTups, self.charToDex, \"unk\")\n",
    "\n",
    "                # Step 3. Run our forward pass.\n",
    "                tag_scores = self.model(sentence_in, charEmbeds)\n",
    "\n",
    "                # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "                #  calling optimizer.step()\n",
    "                loss = loss_function(tag_scores, targets)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            if epoch == self.epochs-1:\n",
    "                epoch_str = '' # last epoch so do not use epoch number in model filename\n",
    "            else:\n",
    "                epoch_str = str(epoch)\n",
    "            savefile = self.modelfile + epoch_str + self.modelsuffix\n",
    "            print(\"saving model file: {}\".format(savefile), file=sys.stderr)\n",
    "            torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': self.model.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'loss': loss,\n",
    "                        'unk': self.unk,\n",
    "                        'word_to_ix': self.word_to_ix,\n",
    "                        'tag_to_ix': self.tag_to_ix,\n",
    "                        'ix_to_tag': self.ix_to_tag,\n",
    "                        'char_to_dex': self.charToDex\n",
    "                    }, savefile)\n",
    "\n",
    "    def decode(self, inputfile):\n",
    "        if inputfile[-3:] == '.gz':\n",
    "            with gzip.open(inputfile, 'rt') as f:\n",
    "                input_data = read_conll(f, input_idx=0, label_idx=-1)\n",
    "        else:\n",
    "            with open(inputfile, 'r') as f:\n",
    "                input_data = read_conll(f, input_idx=0, label_idx=-1)\n",
    "\n",
    "        if not os.path.isfile(self.modelfile + self.modelsuffix):\n",
    "            raise IOError(\"Error: missing model file {}\".format(self.modelfile + self.modelsuffix))\n",
    "\n",
    "        saved_model = torch.load(self.modelfile + self.modelsuffix)\n",
    "        self.model.load_state_dict(saved_model['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(saved_model['optimizer_state_dict'])\n",
    "        epoch = saved_model['epoch']\n",
    "        loss = saved_model['loss']\n",
    "        self.unk = saved_model['unk']\n",
    "        self.word_to_ix = saved_model['word_to_ix']\n",
    "        self.tag_to_ix = saved_model['tag_to_ix']\n",
    "        self.ix_to_tag = saved_model['ix_to_tag']\n",
    "        self.charToDex = saved_model['char_to_dex']\n",
    "        self.model.eval()\n",
    "        print(\"Decoding\")\n",
    "        decoder_output = []\n",
    "        for sent, charTups in tqdm.tqdm(input_data):\n",
    "            #print(sent)\n",
    "            decoder_output.append(self.argmax(sent, charTups))\n",
    "        return decoder_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/8936 [00:00<02:20, 63.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Modified Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8936/8936 [01:11<00:00, 124.48it/s]\n",
      "saving model file: charEmbedBase0.tar\n",
      "100%|██████████| 8936/8936 [01:15<00:00, 118.42it/s]\n",
      "saving model file: charEmbedBase1.tar\n",
      "100%|██████████| 8936/8936 [01:15<00:00, 118.31it/s]\n",
      "saving model file: charEmbedBase2.tar\n",
      "100%|██████████| 8936/8936 [01:15<00:00, 118.27it/s]\n",
      "saving model file: charEmbedBase3.tar\n",
      "100%|██████████| 8936/8936 [01:18<00:00, 114.07it/s]\n",
      "saving model file: charEmbedBase4.tar\n",
      "100%|██████████| 8936/8936 [01:13<00:00, 121.98it/s]\n",
      "saving model file: charEmbedBase5.tar\n",
      "100%|██████████| 8936/8936 [01:13<00:00, 121.26it/s]\n",
      "saving model file: charEmbedBase6.tar\n",
      "100%|██████████| 8936/8936 [01:22<00:00, 107.88it/s]\n",
      "saving model file: charEmbedBase7.tar\n",
      "100%|██████████| 8936/8936 [01:14<00:00, 119.30it/s]\n",
      "saving model file: charEmbedBase8.tar\n",
      "100%|██████████| 8936/8936 [01:06<00:00, 135.09it/s]\n",
      "saving model file: charEmbedBase.tar\n"
     ]
    }
   ],
   "source": [
    "trainfile = os.path.join('../data', 'train.txt.gz')\n",
    "modelsuffix = '.tar'\n",
    "unk = '[UNK]'\n",
    "\n",
    "chunker = LSTMTagger(trainfile, \"charEmbedBase\", modelsuffix, unk)\n",
    "chunker.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 71/1027 [00:00<00:01, 707.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1027/1027 [00:01<00:00, 706.24it/s]\n"
     ]
    }
   ],
   "source": [
    "decoder_output = chunker.decode('../data/input/dev.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 23663 tokens with 11896 phrases; found: 11983 phrases; correct: 9206.\n",
      "accuracy:  86.85%; (non-O)\n",
      "accuracy:  87.88%; precision:  76.83%; recall:  77.39%; FB1:  77.11\n",
      "             ADJP: precision:  43.33%; recall:  17.26%; FB1:  24.68  90\n",
      "             ADVP: precision:  66.55%; recall:  46.48%; FB1:  54.73  278\n",
      "            CONJP: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "             INTJ: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "               NP: precision:  75.71%; recall:  80.49%; FB1:  78.02  6631\n",
      "               PP: precision:  93.03%; recall:  86.44%; FB1:  89.62  2268\n",
      "              PRT: precision:  65.22%; recall:  66.67%; FB1:  65.93  46\n",
      "             SBAR: precision:  81.54%; recall:  44.73%; FB1:  57.77  130\n",
      "               VP: precision:  67.56%; recall:  74.48%; FB1:  70.85  2540\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(76.82550279562714, 77.38735709482178, 77.1054064240546)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_output = [ output for sent in decoder_output for output in sent ]\n",
    "import conlleval\n",
    "true_seqs = []\n",
    "with open(os.path.join('../data','reference','dev.out')) as r:\n",
    "    for sent in conlleval.read_file(r):\n",
    "        true_seqs += sent.split()\n",
    "conlleval.evaluate(true_seqs, flat_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character Embeddings this time. Makes more sense I think. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTaggerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, char_size):\n",
    "        torch.manual_seed(1)\n",
    "        super(LSTMTaggerModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.char_embeddings = nn.Parameter(torch.zeros(char_size, embedding_dim))\n",
    "        \n",
    "        torch.nn.init.normal_(self.char_embeddings)\n",
    "        \n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        # If sum chars\n",
    "        # self.lstm = nn.LSTM(embedding_dim * 2, hidden_dim, bidirectional=False)\n",
    "        # If 4 unique embeds\n",
    "        self.lstm = nn.LSTM(embedding_dim * 4, hidden_dim, bidirectional=False)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence, charEmbed):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        #print(embeds.shape)\n",
    "        # Not sure what embedding dim is, whether [B, Max Set Len, Embed dim] or something else???\n",
    "        charEmbeds = torch.matmul(charEmbed, self.char_embeddings)\n",
    "        #print(charEmbeds.shape)\n",
    "        #charEmbeds = charEmbeds.view(int(charEmbeds.shape[0] / 3), 3, charEmbeds.shape[1]).sum(1)\n",
    "        \n",
    "        # Concat all this time\n",
    "        charEmbeds = charEmbeds.view(int(charEmbeds.shape[0] / 3), 3, charEmbeds.shape[1])\n",
    "        charEmbeds = charEmbeds.view(charEmbeds.shape[0], -1)\n",
    "        \n",
    "        #print(charEmbeds.shape)\n",
    "        embeds = torch.cat([embeds, charEmbeds],-1)\n",
    "        #print(embeds.shape)\n",
    "        #print(\"Concat Done\")\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "\n",
    "class LSTMTagger:\n",
    "\n",
    "    def __init__(self, trainfile, modelfile, modelsuffix, unk=\"[UNK]\", epochs=10, embedding_dim=128, hidden_dim=64):\n",
    "        self.unk = unk\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.epochs = epochs\n",
    "        self.modelfile = modelfile\n",
    "        self.modelsuffix = modelsuffix\n",
    "        self.training_data = []\n",
    "        if trainfile[-3:] == '.gz':\n",
    "            with gzip.open(trainfile, 'rt') as f:\n",
    "                self.training_data = read_conll(f)\n",
    "        else:\n",
    "            with open(trainfile, 'r') as f:\n",
    "                self.training_data = read_conll(f)\n",
    "\n",
    "        self.word_to_ix = {} # replaces words with an index (one-hot vector)\n",
    "        self.tag_to_ix = {} # replace output labels / tags with an index\n",
    "        self.ix_to_tag = [] # during inference we produce tag indices so we have to map it back to a tag\n",
    "\n",
    "        chars = set()\n",
    "        for sent, tags, charTups in self.training_data:\n",
    "            \n",
    "            for word in sent:\n",
    "                if word not in self.word_to_ix:\n",
    "                    self.word_to_ix[word] = len(self.word_to_ix)\n",
    "                    \n",
    "            for tag in tags:\n",
    "                if tag not in self.tag_to_ix:\n",
    "                    self.tag_to_ix[tag] = len(self.tag_to_ix)\n",
    "                    self.ix_to_tag.append(tag)\n",
    "                    \n",
    "            for ct in charTups:\n",
    "    \n",
    "                chars.add(ct[0])\n",
    "\n",
    "                if ct[2] is not None:\n",
    "                    chars.add(ct[2])\n",
    "\n",
    "                if ct[1] is not None:\n",
    "                    for char in ct[1]:\n",
    "                        chars.add(char)\n",
    "\n",
    "        chars = list(chars)\n",
    "        chars = sorted(chars)\n",
    "        charToDex = {char: dex for dex, char in enumerate(chars)}\n",
    "        charToDex['unk'] = len(charToDex)\n",
    "        self.charToDex = charToDex\n",
    "        \n",
    "        logging.info(\"word_to_ix:\", self.word_to_ix)\n",
    "        logging.info(\"tag_to_ix:\", self.tag_to_ix)\n",
    "        logging.info(\"ix_to_tag:\", self.ix_to_tag)\n",
    "        logging.info(\"char_to_dex:\", self.charToDex)\n",
    "        \n",
    "        print(\"Creating Modified Model\")\n",
    "        self.model = LSTMTaggerModel(self.embedding_dim, self.hidden_dim, len(self.word_to_ix), len(self.tag_to_ix), len(self.charToDex))\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=0.01)\n",
    "\n",
    "    def argmax(self, seq, charTups):\n",
    "        output = []\n",
    "        with torch.no_grad():\n",
    "            inputs = prepare_sequence(seq, self.word_to_ix, self.unk)\n",
    "            charEmbeds = prepare_sequence(charTups, self.charToDex, \"unk\")\n",
    "            tag_scores = self.model(inputs, charEmbeds)\n",
    "            for i in range(len(inputs)):\n",
    "                output.append(self.ix_to_tag[int(tag_scores[i].argmax(dim=0))])\n",
    "        return output\n",
    "\n",
    "    def train(self):\n",
    "        loss_function = nn.NLLLoss()\n",
    "\n",
    "        self.model.train()\n",
    "        loss = float(\"inf\")\n",
    "        for epoch in range(self.epochs):\n",
    "            for sentence, tags, charTups in tqdm.tqdm(self.training_data):\n",
    "                # Step 1. Remember that Pytorch accumulates gradients.\n",
    "                # We need to clear them out before each instance\n",
    "                self.model.zero_grad()\n",
    "\n",
    "                # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "                # Tensors of word indices.\n",
    "                sentence_in = prepare_sequence(sentence, self.word_to_ix, self.unk)\n",
    "                targets = prepare_sequence(tags, self.tag_to_ix, self.unk)\n",
    "                charEmbeds = prepare_sequence(charTups, self.charToDex, \"unk\")\n",
    "\n",
    "                # Step 3. Run our forward pass.\n",
    "                tag_scores = self.model(sentence_in, charEmbeds)\n",
    "\n",
    "                # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "                #  calling optimizer.step()\n",
    "                loss = loss_function(tag_scores, targets)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            if epoch == self.epochs-1:\n",
    "                epoch_str = '' # last epoch so do not use epoch number in model filename\n",
    "            else:\n",
    "                epoch_str = str(epoch)\n",
    "            savefile = self.modelfile + epoch_str + self.modelsuffix\n",
    "            print(\"saving model file: {}\".format(savefile), file=sys.stderr)\n",
    "            torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': self.model.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'loss': loss,\n",
    "                        'unk': self.unk,\n",
    "                        'word_to_ix': self.word_to_ix,\n",
    "                        'tag_to_ix': self.tag_to_ix,\n",
    "                        'ix_to_tag': self.ix_to_tag,\n",
    "                        'char_to_dex': self.charToDex\n",
    "                    }, savefile)\n",
    "\n",
    "    def decode(self, inputfile):\n",
    "        if inputfile[-3:] == '.gz':\n",
    "            with gzip.open(inputfile, 'rt') as f:\n",
    "                input_data = read_conll(f, input_idx=0, label_idx=-1)\n",
    "        else:\n",
    "            with open(inputfile, 'r') as f:\n",
    "                input_data = read_conll(f, input_idx=0, label_idx=-1)\n",
    "\n",
    "        if not os.path.isfile(self.modelfile + self.modelsuffix):\n",
    "            raise IOError(\"Error: missing model file {}\".format(self.modelfile + self.modelsuffix))\n",
    "\n",
    "        saved_model = torch.load(self.modelfile + self.modelsuffix)\n",
    "        self.model.load_state_dict(saved_model['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(saved_model['optimizer_state_dict'])\n",
    "        epoch = saved_model['epoch']\n",
    "        loss = saved_model['loss']\n",
    "        self.unk = saved_model['unk']\n",
    "        self.word_to_ix = saved_model['word_to_ix']\n",
    "        self.tag_to_ix = saved_model['tag_to_ix']\n",
    "        self.ix_to_tag = saved_model['ix_to_tag']\n",
    "        self.charToDex = saved_model['char_to_dex']\n",
    "        self.model.eval()\n",
    "        print(\"Decoding\")\n",
    "        decoder_output = []\n",
    "        for sent, charTups in tqdm.tqdm(input_data):\n",
    "            #print(sent)\n",
    "            decoder_output.append(self.argmax(sent, charTups))\n",
    "        return decoder_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/8936 [00:00<01:43, 86.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Modified Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8936/8936 [01:08<00:00, 130.40it/s]\n",
      "saving model file: charEmbedMod.0.tar\n",
      "100%|██████████| 8936/8936 [01:14<00:00, 120.57it/s]\n",
      "saving model file: charEmbedMod.1.tar\n",
      "100%|██████████| 8936/8936 [01:12<00:00, 123.40it/s]\n",
      "saving model file: charEmbedMod.2.tar\n",
      "  6%|▌         | 525/8936 [00:03<01:07, 124.09it/s]"
     ]
    }
   ],
   "source": [
    "trainfile = os.path.join('../data', 'train.txt.gz')\n",
    "modelsuffix = '.tar'\n",
    "unk = '[UNK]'\n",
    "\n",
    "chunker = LSTMTagger(trainfile, \"charEmbedMod.\", modelsuffix, unk)\n",
    "chunker.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "decoder_output = chunker.decode('../data/input/dev.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_output = [ output for sent in decoder_output for output in sent ]\n",
    "import conlleval\n",
    "true_seqs = []\n",
    "with open(os.path.join('../data','reference','dev.out')) as r:\n",
    "    for sent in conlleval.read_file(r):\n",
    "        true_seqs += sent.split()\n",
    "conlleval.evaluate(true_seqs, flat_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
