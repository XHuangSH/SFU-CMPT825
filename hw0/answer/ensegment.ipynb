{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensegment: Punish Longer Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ensegment import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "First let's run the regular version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choose spain\n",
      "this is a test\n",
      "who represents\n",
      "experts exchange\n",
      "speed of art\n",
      "unclimatechangebody\n",
      "we are the people\n",
      "mentionyourfaves\n",
      "now playing\n",
      "the walking dead\n",
      "follow me\n",
      "we are the people\n",
      "mentionyourfaves\n",
      "check domain\n",
      "big rock\n",
      "name cheap\n",
      "apple domains\n",
      "honesty hour\n",
      "being human\n",
      "follow back\n",
      "social media\n",
      "30secondstoearth\n",
      "current ratesoughttogodown\n",
      "this is insane\n",
      "what is my name\n",
      "is it time\n",
      "let us go\n",
      "me too\n",
      "nowthatcherisdead\n",
      "advice for young journalists\n"
     ]
    }
   ],
   "source": [
    "Pw = Pdist(data=datafile(\"../data/count_1w.txt\"))\n",
    "segmenter = Segment(Pw)\n",
    "with open(\"../data/input/dev.txt\") as f:\n",
    "    for line in f:\n",
    "        print(\" \".join(segmenter.segment(line.strip())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Some longer words should be split. Perhaps we should focus on what to do for out of vocab words. Lets add the function below which punishes longer words by increasing the demoninator size with respect to word length. Here we first try using 10 since it is the one used in the referenced book chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punishLong(word, N):\n",
    "\n",
    "    \"Estimate the prob of unknown word while accounting for word length\"\n",
    "    return 10. / (N * 10**len(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choose spain\n",
      "this is a test\n",
      "who represents\n",
      "experts exchange\n",
      "speed of art\n",
      "un climate change body\n",
      "we are the people\n",
      "mention your faves\n",
      "now playing\n",
      "the walking dead\n",
      "follow me\n",
      "we are the people\n",
      "mention your faves\n",
      "check domain\n",
      "big rock\n",
      "name cheap\n",
      "apple domains\n",
      "honesty hour\n",
      "being human\n",
      "follow back\n",
      "social media\n",
      "30 seconds to earth\n",
      "current rate sought to go down\n",
      "this is insane\n",
      "what is my name\n",
      "is it time\n",
      "let us go\n",
      "me too\n",
      "now thatcher is dead\n",
      "advice for young journalists\n"
     ]
    }
   ],
   "source": [
    "Pw = Pdist(data=datafile(\"../data/count_1w.txt\"), missingfn=punishLong)\n",
    "segmenter = Segment(Pw)\n",
    "with open(\"../data/input/dev.txt\") as f:\n",
    "    for line in f:\n",
    "        print(\" \".join(segmenter.segment(line.strip())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks much better now. Lets check the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how to breakup in 5 words\n",
      "what makes god smile\n",
      "10 people who mean alot to me\n",
      "worst day in 4 words\n",
      "love story in 5 words\n",
      "to p3 favourite comics\n",
      "10 breakup lines\n",
      "things that make you smile\n",
      "best female athlete\n",
      "worst bossin5 words\n",
      "now is the time for all good\n",
      "it is a truth universally acknowledged\n",
      "when in the course of human events it becomes necessary\n",
      "it was a bright cold day in april and the clocks were striking thirteen\n",
      "it was the best of times it was the worst of times it was the age of wisdom it was the age of foolishness\n",
      "as gregor samsa awoke one morning from uneasy dreams he found himself transformed in his bed into a gigantic insect\n",
      "in a hole in the ground there lived a hobbit not a nasty dirty wet hole filled with the ends of worms and an oozy smell nor yet a dry bare sandy hole with nothing in it to sitdown on or to eat it was a hobbit hole and that means comfort\n",
      "far out in the uncharted backwaters of the unfashionable end of the western spiral arm of the galaxy lies a small un regarded yellow sun\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/input/test.txt\") as f:\n",
    "    for line in f:\n",
    "        print(\" \".join(segmenter.segment(line.strip())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theres still some errors. At least 2 involving numbers, for example we have to p3 instead of top 3 and bossin5... Maybe a larger penalty for word length will help? Let's see what will happen when we replace 10 with 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how to breakup in 5 words\n",
      "what makes god smile\n",
      "10 people who mean alot to me\n",
      "worst day in 4 words\n",
      "love story in 5 words\n",
      "top 3 favourite comics\n",
      "10 breakup lines\n",
      "things that make you smile\n",
      "best female athlete\n",
      "worst boss in 5 words\n",
      "now is the time for all good\n",
      "it is a truth universally acknowledged\n",
      "when in the course of human events it becomes necessary\n",
      "it was a bright cold day in april and the clocks were striking thirteen\n",
      "it was the best of times it was the worst of times it was the age of wisdom it was the age of foolishness\n",
      "as gregor samsa awoke one morning from uneasy dreams he found himself transformed in his bed into a gigantic insect\n",
      "in a hole in the ground there lived a hobbit not a nasty dirty wet hole filled with the ends of worms and an oozy smell nor yet a dry bare sandy hole with nothing in it to sitdown on or to eat it was a hobbit hole and that means comfort\n",
      "far out in the uncharted backwaters of the unfashionable end of the western spiral arm of the galaxy lies a small un regarded yellow sun\n"
     ]
    }
   ],
   "source": [
    "def punishLong(word, N):\n",
    "\n",
    "    \"Estimate the prob of unknown word while accounting for word length\"\n",
    "    return 50. / (N * 50**len(word))\n",
    "\n",
    "Pw = Pdist(data=datafile(\"../data/count_1w.txt\"), missingfn=punishLong)\n",
    "segmenter = Segment(Pw)\n",
    "with open(\"../data/input/test.txt\") as f:\n",
    "    for line in f:\n",
    "        print(\" \".join(segmenter.segment(line.strip())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks better than before, to p3 is now top 3. However, un regarded should be a single word. Otherwise it looks great. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
