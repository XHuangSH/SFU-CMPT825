{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline -- Retrofitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lexsub import *\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import copy\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We first need to write a function compute all the neighbor words according to the lexicons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## lexFiles: can be a list of files\n",
    "## return: neighboring words for each word \n",
    "## add all pairwise related neighbors\n",
    "def get_neighbor_ws(lexFiles):\n",
    "    word2neigh = {}\n",
    "    accountedFor = set()\n",
    "    for lexDex, lexF in enumerate(lexFiles):\n",
    "        print(\"On [{}/{}]\".format(lexDex, len(lexFiles)))\n",
    "        with open(lexF, 'r') as f:\n",
    "            for line in f:\n",
    "                cWords = line.lower().strip().split(' ')\n",
    "                for i in range(len(cWords)): \n",
    "                    w1 = cWords[i]  # current word\n",
    "                    for j in range(i+1, len(cWords)):\n",
    "                        w2 = cWords[j]  # later word\n",
    "                        if w1 not in word2neigh:  \n",
    "                            word2neigh[w1] = [w2]\n",
    "                        else:\n",
    "                            if w2 not in word2neigh[w1]:\n",
    "                                word2neigh[w1].append(w2)\n",
    "                        if w2 not in word2neigh:  \n",
    "                            word2neigh[w2] = [w1]\n",
    "                        else:\n",
    "                            if w1 not in word2neigh[w2]:\n",
    "                                word2neigh[w2].append(w1)\n",
    "    return word2neigh\n",
    "                              \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some quick test show that the get adjacent word function is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/lexicons/ppdb-xl.txt']\n",
      "On [0/1]\n",
      "['jolly', 'jolie', 'merry', 'julie', 'nice', 'beautiful', 'pretty', 'sweet', 'lovely', 'pleasant', 'good', 'cute', 'handsome', 'fun', 'cool', 'belle', 'fine', 'enjoyable', 'gentle', 'good-looking', 'bel', 'beau', 'charming', 'well', 'owl', 'neat', 'joli', 'great', 'sympathetic', 'agreeable', 'pleased', 'gentil', 'kind', 'chouette', 'jamil', 'delighted', '-nice', 'beautifui', 'nice-looking', 'ravi', 'super', 'wonderful', 'fancy', 'pleasure', 'gorgeous', 'glad', 'satisfied', 'gratified', 'welcome', 'proud', 'grateful', 'thrilled', 'commends', 'excited', 'congratulated', 'fortunate', 'welcomed', 'gratifying', 'content', 'cheery', 'charmed', '-merry', '-happy', 'bonheur', 'happiness', 'horny', 'psyched', 'enthusiastic', 'packaged', 'aroused', 'impatient', 'exciting', 'excite', 'febrile', 'agitated', 'nervous', 'eager', 'pumped', 'joyous', 'jubilant', 'joyful', 'festive', 'pleasurable', 'pleasing', 'congenial', 'comfortable', 'delightful', 'happily', 'fortunately', 'cheerfully', 'thankfully', 'luckily', 'gladly', 'blithely', 'pleasantly', 'merrily', 'gaily', 'willingly', 'mercifully', 'commend', 'applaud', 'welcoming', 'welcomes', 'bienvenue', 'salute', 'bienvenido', 'appreciate', 'greet', 'congratulate', 'positive', 'välkommen', 'bienvenidos', 'bienvenida', 'rejoice', 'host', 'commendable', '-welcome', 'home', 'desirable', 'favourably', 'lucky', 'hopefully', 'convinced', 'met', 'fulfilled', 'persuaded', 'confident', 'satisfies', 'satisfactory', 'contented', 'satisfaction', 'complacent', 'filled', 'respected', 'satisﬁed', 'joy', 'bliss', 'luck', 'well-being', 'fuk', 'delight', 'excellency', 'saeed', 'saed', 'saïd', 'saied', \"sa'id\", 'said', 'syed', 'upbeat', 'sayeed', 'sayed', 'cheerful', 'smiley', 'happy-go-lucky', 'pippin', 'gay', 'chipper', 'al-said', 'al-saeed', 'al-sayyed', 'happier', 'asaad', 'feliz', 'plaisance', 'recreational', 'placentia', 'honour', 'amusement', 'placer', 'gusto', 'enjoyment', 'placing', 'place', 'recreation', 'thrill', \"'heureux\", 'successful', 'success', 'fruitful', 'successfully', 'prosperous', 'effective', 'succeeded', 'productive', 'smooth', 'chengkung', 'achievement', 'succeed', 'efficient', 'successes', 'conclusive', 'thriving', 'thankful', 'recognizing', 'appreciative', 'acknowledging', 'indebted', 'obliged', 'recognising', 'felicitous', 'playful', 'cree', 'iucky', 'chance', 'contents', 'substance', 'contained', 'contenu', 'contenido', 'tenor', 'elements', 'connotation', 'material', 'concentration', 'buenos', 'sound', 'right', 'excellent', 'proper', 'okay', 'bon', 'correct', 'boa', 'hassan', 'maid', 'best', 'useful', 'ok', 'bona', 'bra', 'boas', 'bonne', 'favourable', 'healthy', 'favorable', 'buena', 'beneficial', '-good', 'guten', 'valid', 'properly', 'appropriate', 'bueno', 'quality', 'perfect', 'buen', 'bom', 'alright', 'tayyip', 'bun', 'al-rashid', 'strong', 'buon', 'solid', 'decent', 'adequate', 'bills', 'jesus', 'coupons', 'brave', 'much', 'hasan', 'al-khair', 'al-rasheed', 'property', 'optimistic', 'bullish', 'buoyant', 'auspicious', 'hopeful', 'privileged', 'wealthy']\n"
     ]
    }
   ],
   "source": [
    "lexFiles = glob.glob('../data/lexicons/ppdb-xl.txt')\n",
    "print(lexFiles)\n",
    "word2neigh = get_neighbor_ws(lexFiles)\n",
    "print(word2neigh['happy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need to design the retrofit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## wVecs: word vectors\n",
    "## word2Neigh: adjacent words according to lexicons\n",
    "## beta, alpha: hyper-parameters\n",
    "## numIters: number of iterations\n",
    "def retrofit(wVecs, word2Neigh, beta = 1.0, alpha = 1.0, numIters = 10):\n",
    "    \n",
    "    givenVocab = set()\n",
    "    for word, vect in wVecs:\n",
    "        givenVocab.add(word)\n",
    "\n",
    "    # Need to create a modifiable version of the vectors as pymag is read only??!?!?!?\n",
    "    newEmbs = {}\n",
    "    for word, emb in wVecs:\n",
    "        newEmbs[word] = emb\n",
    "        \n",
    "    for itCount in range(numIters):\n",
    "        for wCount, gWord in enumerate(givenVocab):\n",
    "            if (wCount % 100000) == 0:\n",
    "                print(\"Iter [{}/{}] Word [{}/{}]\".format(itCount, numIters, wCount, len(givenVocab)))\n",
    "          \n",
    "            tmpEmb = np.zeros(newEmbs[gWord].shape)\n",
    "            if gWord in word2Neigh:\n",
    "                nCount = 0\n",
    "                cLoopVocab = word2Neigh[gWord]\n",
    "                for word in cLoopVocab:\n",
    "                    if word in newEmbs:\n",
    "                        tmpEmb += beta * newEmbs[word]\n",
    "                        nCount += 1\n",
    "                \n",
    "                newEmbs[gWord] = ((tmpEmb + (alpha * wVecs.query(gWord)))) / (nCount + alpha)\n",
    "    \n",
    "    return newEmbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let us use retrofitting to train a new set of word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter [0/10] Word [0/400000]\n",
      "Iter [0/10] Word [100000/400000]\n",
      "Iter [0/10] Word [200000/400000]\n",
      "Iter [0/10] Word [300000/400000]\n",
      "Iter [1/10] Word [0/400000]\n",
      "Iter [1/10] Word [100000/400000]\n",
      "Iter [1/10] Word [200000/400000]\n",
      "Iter [1/10] Word [300000/400000]\n",
      "Iter [2/10] Word [0/400000]\n",
      "Iter [2/10] Word [100000/400000]\n",
      "Iter [2/10] Word [200000/400000]\n",
      "Iter [2/10] Word [300000/400000]\n",
      "Iter [3/10] Word [0/400000]\n",
      "Iter [3/10] Word [100000/400000]\n",
      "Iter [3/10] Word [200000/400000]\n",
      "Iter [3/10] Word [300000/400000]\n",
      "Iter [4/10] Word [0/400000]\n",
      "Iter [4/10] Word [100000/400000]\n",
      "Iter [4/10] Word [200000/400000]\n",
      "Iter [4/10] Word [300000/400000]\n",
      "Iter [5/10] Word [0/400000]\n",
      "Iter [5/10] Word [100000/400000]\n",
      "Iter [5/10] Word [200000/400000]\n",
      "Iter [5/10] Word [300000/400000]\n",
      "Iter [6/10] Word [0/400000]\n",
      "Iter [6/10] Word [100000/400000]\n",
      "Iter [6/10] Word [200000/400000]\n",
      "Iter [6/10] Word [300000/400000]\n",
      "Iter [7/10] Word [0/400000]\n",
      "Iter [7/10] Word [100000/400000]\n",
      "Iter [7/10] Word [200000/400000]\n",
      "Iter [7/10] Word [300000/400000]\n",
      "Iter [8/10] Word [0/400000]\n",
      "Iter [8/10] Word [100000/400000]\n",
      "Iter [8/10] Word [200000/400000]\n",
      "Iter [8/10] Word [300000/400000]\n",
      "Iter [9/10] Word [0/400000]\n",
      "Iter [9/10] Word [100000/400000]\n",
      "Iter [9/10] Word [200000/400000]\n",
      "Iter [9/10] Word [300000/400000]\n"
     ]
    }
   ],
   "source": [
    "senseMag = LexSub(os.path.join('../../','glove.6B.100d.magnitude'))\n",
    "senseVecs = senseMag.wvecs\n",
    "newWordVects = retrofit(senseVecs, word2neigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"newWordVects_wn.npy\", newWordVects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to Txt [0/400000]\n",
      "Writing to Txt [5000/400000]\n",
      "Writing to Txt [10000/400000]\n",
      "Writing to Txt [15000/400000]\n",
      "Writing to Txt [20000/400000]\n",
      "Writing to Txt [25000/400000]\n",
      "Writing to Txt [30000/400000]\n",
      "Writing to Txt [35000/400000]\n",
      "Writing to Txt [40000/400000]\n",
      "Writing to Txt [45000/400000]\n",
      "Writing to Txt [50000/400000]\n",
      "Writing to Txt [55000/400000]\n",
      "Writing to Txt [60000/400000]\n",
      "Writing to Txt [65000/400000]\n",
      "Writing to Txt [70000/400000]\n",
      "Writing to Txt [75000/400000]\n",
      "Writing to Txt [80000/400000]\n",
      "Writing to Txt [85000/400000]\n",
      "Writing to Txt [90000/400000]\n",
      "Writing to Txt [95000/400000]\n",
      "Writing to Txt [100000/400000]\n",
      "Writing to Txt [105000/400000]\n",
      "Writing to Txt [110000/400000]\n",
      "Writing to Txt [115000/400000]\n",
      "Writing to Txt [120000/400000]\n",
      "Writing to Txt [125000/400000]\n",
      "Writing to Txt [130000/400000]\n",
      "Writing to Txt [135000/400000]\n",
      "Writing to Txt [140000/400000]\n",
      "Writing to Txt [145000/400000]\n",
      "Writing to Txt [150000/400000]\n",
      "Writing to Txt [155000/400000]\n",
      "Writing to Txt [160000/400000]\n",
      "Writing to Txt [165000/400000]\n",
      "Writing to Txt [170000/400000]\n",
      "Writing to Txt [175000/400000]\n",
      "Writing to Txt [180000/400000]\n",
      "Writing to Txt [185000/400000]\n",
      "Writing to Txt [190000/400000]\n",
      "Writing to Txt [195000/400000]\n",
      "Writing to Txt [200000/400000]\n",
      "Writing to Txt [205000/400000]\n",
      "Writing to Txt [210000/400000]\n",
      "Writing to Txt [215000/400000]\n",
      "Writing to Txt [220000/400000]\n",
      "Writing to Txt [225000/400000]\n",
      "Writing to Txt [230000/400000]\n",
      "Writing to Txt [235000/400000]\n",
      "Writing to Txt [240000/400000]\n",
      "Writing to Txt [245000/400000]\n",
      "Writing to Txt [250000/400000]\n",
      "Writing to Txt [255000/400000]\n",
      "Writing to Txt [260000/400000]\n",
      "Writing to Txt [265000/400000]\n",
      "Writing to Txt [270000/400000]\n",
      "Writing to Txt [275000/400000]\n",
      "Writing to Txt [280000/400000]\n",
      "Writing to Txt [285000/400000]\n",
      "Writing to Txt [290000/400000]\n",
      "Writing to Txt [295000/400000]\n",
      "Writing to Txt [300000/400000]\n",
      "Writing to Txt [305000/400000]\n",
      "Writing to Txt [310000/400000]\n",
      "Writing to Txt [315000/400000]\n",
      "Writing to Txt [320000/400000]\n",
      "Writing to Txt [325000/400000]\n",
      "Writing to Txt [330000/400000]\n",
      "Writing to Txt [335000/400000]\n",
      "Writing to Txt [340000/400000]\n",
      "Writing to Txt [345000/400000]\n",
      "Writing to Txt [350000/400000]\n",
      "Writing to Txt [355000/400000]\n",
      "Writing to Txt [360000/400000]\n",
      "Writing to Txt [365000/400000]\n",
      "Writing to Txt [370000/400000]\n",
      "Writing to Txt [375000/400000]\n",
      "Writing to Txt [380000/400000]\n",
      "Writing to Txt [385000/400000]\n",
      "Writing to Txt [390000/400000]\n",
      "Writing to Txt [395000/400000]\n",
      "Written\n"
     ]
    }
   ],
   "source": [
    "with open(\"newVects_wn.txt\", \"w\") as f:\n",
    "    count = 0\n",
    "    vSize = len(newWordVects)\n",
    "    for word, emb in newWordVects.items():\n",
    "        if (count % 5000) == 0:\n",
    "            print(\"Writing to Txt [{}/{}]\".format(count, vSize))\n",
    "            \n",
    "        f.write(word)\n",
    "        for num in emb:\n",
    "            f.write(\" \" + str(num))\n",
    "        f.write(\"\\n\")\n",
    "        count += 1\n",
    "\n",
    "print(\"Written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "subprocess.call([\"python\", \"-m\", \"pymagnitude.converter\", \"-i\", \"newVects_wn.txt\", \"-o\", \"newVects_wn.magnitude\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on the new word vectors give us a score of 46.80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score=46.80\n"
     ]
    }
   ],
   "source": [
    "from lexsub_check import precision\n",
    "\n",
    "lexsub = LexSub('newVects_wn.magnitude')\n",
    "output = []\n",
    "with open(os.path.join('../data','input','dev.txt')) as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        # fields is [index, sentence]\n",
    "        output.append(\" \".join(lexsub.substitutes(int(fields[0].strip()), fields[1].strip().split())))\n",
    "\n",
    "with open(os.path.join('../data','reference','dev.out'), 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "print(\"Score={:.2f}\".format(100*precision(ref_data, output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We use a simple grid-search method for finetuning the hyper-parameters and find that beta = 1.0, alpha = 2.0 give us slightly higher score of 47.45."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter [0/10] Word [0/400000]\n",
      "Iter [0/10] Word [100000/400000]\n",
      "Iter [0/10] Word [200000/400000]\n",
      "Iter [0/10] Word [300000/400000]\n",
      "Iter [1/10] Word [0/400000]\n",
      "Iter [1/10] Word [100000/400000]\n",
      "Iter [1/10] Word [200000/400000]\n",
      "Iter [1/10] Word [300000/400000]\n",
      "Iter [2/10] Word [0/400000]\n",
      "Iter [2/10] Word [100000/400000]\n",
      "Iter [2/10] Word [200000/400000]\n",
      "Iter [2/10] Word [300000/400000]\n",
      "Iter [3/10] Word [0/400000]\n",
      "Iter [3/10] Word [100000/400000]\n",
      "Iter [3/10] Word [200000/400000]\n",
      "Iter [3/10] Word [300000/400000]\n",
      "Iter [4/10] Word [0/400000]\n",
      "Iter [4/10] Word [100000/400000]\n",
      "Iter [4/10] Word [200000/400000]\n",
      "Iter [4/10] Word [300000/400000]\n",
      "Iter [5/10] Word [0/400000]\n",
      "Iter [5/10] Word [100000/400000]\n",
      "Iter [5/10] Word [200000/400000]\n",
      "Iter [5/10] Word [300000/400000]\n",
      "Iter [6/10] Word [0/400000]\n",
      "Iter [6/10] Word [100000/400000]\n",
      "Iter [6/10] Word [200000/400000]\n",
      "Iter [6/10] Word [300000/400000]\n",
      "Iter [7/10] Word [0/400000]\n",
      "Iter [7/10] Word [100000/400000]\n",
      "Iter [7/10] Word [200000/400000]\n",
      "Iter [7/10] Word [300000/400000]\n",
      "Iter [8/10] Word [0/400000]\n",
      "Iter [8/10] Word [100000/400000]\n",
      "Iter [8/10] Word [200000/400000]\n",
      "Iter [8/10] Word [300000/400000]\n",
      "Iter [9/10] Word [0/400000]\n",
      "Iter [9/10] Word [100000/400000]\n",
      "Iter [9/10] Word [200000/400000]\n",
      "Iter [9/10] Word [300000/400000]\n",
      "Writing to Txt [0/400000]\n",
      "Writing to Txt [5000/400000]\n",
      "Writing to Txt [10000/400000]\n",
      "Writing to Txt [15000/400000]\n",
      "Writing to Txt [20000/400000]\n",
      "Writing to Txt [25000/400000]\n",
      "Writing to Txt [30000/400000]\n",
      "Writing to Txt [35000/400000]\n",
      "Writing to Txt [40000/400000]\n",
      "Writing to Txt [45000/400000]\n",
      "Writing to Txt [50000/400000]\n",
      "Writing to Txt [55000/400000]\n",
      "Writing to Txt [60000/400000]\n",
      "Writing to Txt [65000/400000]\n",
      "Writing to Txt [70000/400000]\n",
      "Writing to Txt [75000/400000]\n",
      "Writing to Txt [80000/400000]\n",
      "Writing to Txt [85000/400000]\n",
      "Writing to Txt [90000/400000]\n",
      "Writing to Txt [95000/400000]\n",
      "Writing to Txt [100000/400000]\n",
      "Writing to Txt [105000/400000]\n",
      "Writing to Txt [110000/400000]\n",
      "Writing to Txt [115000/400000]\n",
      "Writing to Txt [120000/400000]\n",
      "Writing to Txt [125000/400000]\n",
      "Writing to Txt [130000/400000]\n",
      "Writing to Txt [135000/400000]\n",
      "Writing to Txt [140000/400000]\n",
      "Writing to Txt [145000/400000]\n",
      "Writing to Txt [150000/400000]\n",
      "Writing to Txt [155000/400000]\n",
      "Writing to Txt [160000/400000]\n",
      "Writing to Txt [165000/400000]\n",
      "Writing to Txt [170000/400000]\n",
      "Writing to Txt [175000/400000]\n",
      "Writing to Txt [180000/400000]\n",
      "Writing to Txt [185000/400000]\n",
      "Writing to Txt [190000/400000]\n",
      "Writing to Txt [195000/400000]\n",
      "Writing to Txt [200000/400000]\n",
      "Writing to Txt [205000/400000]\n",
      "Writing to Txt [210000/400000]\n",
      "Writing to Txt [215000/400000]\n",
      "Writing to Txt [220000/400000]\n",
      "Writing to Txt [225000/400000]\n",
      "Writing to Txt [230000/400000]\n",
      "Writing to Txt [235000/400000]\n",
      "Writing to Txt [240000/400000]\n",
      "Writing to Txt [245000/400000]\n",
      "Writing to Txt [250000/400000]\n",
      "Writing to Txt [255000/400000]\n",
      "Writing to Txt [260000/400000]\n",
      "Writing to Txt [265000/400000]\n",
      "Writing to Txt [270000/400000]\n",
      "Writing to Txt [275000/400000]\n",
      "Writing to Txt [280000/400000]\n",
      "Writing to Txt [285000/400000]\n",
      "Writing to Txt [290000/400000]\n",
      "Writing to Txt [295000/400000]\n",
      "Writing to Txt [300000/400000]\n",
      "Writing to Txt [305000/400000]\n",
      "Writing to Txt [310000/400000]\n",
      "Writing to Txt [315000/400000]\n",
      "Writing to Txt [320000/400000]\n",
      "Writing to Txt [325000/400000]\n",
      "Writing to Txt [330000/400000]\n",
      "Writing to Txt [335000/400000]\n",
      "Writing to Txt [340000/400000]\n",
      "Writing to Txt [345000/400000]\n",
      "Writing to Txt [350000/400000]\n",
      "Writing to Txt [355000/400000]\n",
      "Writing to Txt [360000/400000]\n",
      "Writing to Txt [365000/400000]\n",
      "Writing to Txt [370000/400000]\n",
      "Writing to Txt [375000/400000]\n",
      "Writing to Txt [380000/400000]\n",
      "Writing to Txt [385000/400000]\n",
      "Writing to Txt [390000/400000]\n",
      "Writing to Txt [395000/400000]\n",
      "Written\n",
      "Score=47.45\n"
     ]
    }
   ],
   "source": [
    "newWordVects = retrofit(senseVecs, word2neigh, beta=1.0, alpha=2.0)\n",
    "np.save(\"newWordVects_wn2.npy\", newWordVects)\n",
    "\n",
    "with open(\"newVects_wn2.txt\", \"w\") as f:\n",
    "    count = 0\n",
    "    vSize = len(newWordVects)\n",
    "    for word, emb in newWordVects.items():\n",
    "        if (count % 5000) == 0:\n",
    "            print(\"Writing to Txt [{}/{}]\".format(count, vSize))\n",
    "            \n",
    "        f.write(word)\n",
    "        for num in emb:\n",
    "            f.write(\" \" + str(num))\n",
    "        f.write(\"\\n\")\n",
    "        count += 1\n",
    "\n",
    "print(\"Written\")\n",
    "\n",
    "\n",
    "subprocess.call([\"python\", \"-m\", \"pymagnitude.converter\", \"-i\", \"newVects_wn2.txt\", \"-o\", \"newVects_wn2.magnitude\"])\n",
    "\n",
    "from lexsub_check import precision\n",
    "\n",
    "lexsub = LexSub('newVects_wn2.magnitude')\n",
    "output = []\n",
    "with open(os.path.join('../data','input','dev.txt')) as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        # fields is [index, sentence]\n",
    "        output.append(\" \".join(lexsub.substitutes(int(fields[0].strip()), fields[1].strip().split())))\n",
    "\n",
    "with open(os.path.join('../data','reference','dev.out'), 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "print(\"Score={:.2f}\".format(100*precision(ref_data, output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To improve the performance, we first modify the adjacent word matrix so that it the first word is the anchor and the rest are the neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## lexFiles: can be a list of files\n",
    "## return: neighboring words for each word \n",
    "## add all non-pairwise related neighbors\n",
    "def get_oneside_neighbor_ws(lexFiles):\n",
    "    word2neigh = {}\n",
    "    accountedFor = set()\n",
    "    #lexFiles = [lexFiles[-2]]\n",
    "    #lexFiles = [lexFiles[-2], lexFiles[1]]\n",
    "    for lexDex, lexF in enumerate(lexFiles):\n",
    "        print(\"On [{}/{}]\".format(lexDex, len(lexFiles)))\n",
    "        with open(lexF, 'r') as f:\n",
    "            for line in f:\n",
    "                cWords = line.lower().strip().split(' ')\n",
    "                tmpL = [word for word in cWords[1:]]\n",
    "                if cWords[0] not in word2neigh:\n",
    "                    word2neigh[cWords[0]] = tmpL\n",
    "                else:\n",
    "                    word2neigh[cWords[0]] = word2neigh[cWords[0]] + tmpL\n",
    "\n",
    "    return word2neigh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation score for this verions is 53.20 !!!       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On [0/1]\n",
      "Iter [0/10] Word [0/400000]\n",
      "Iter [0/10] Word [100000/400000]\n",
      "Iter [0/10] Word [200000/400000]\n",
      "Iter [0/10] Word [300000/400000]\n",
      "Iter [1/10] Word [0/400000]\n",
      "Iter [1/10] Word [100000/400000]\n",
      "Iter [1/10] Word [200000/400000]\n",
      "Iter [1/10] Word [300000/400000]\n",
      "Iter [2/10] Word [0/400000]\n",
      "Iter [2/10] Word [100000/400000]\n",
      "Iter [2/10] Word [200000/400000]\n",
      "Iter [2/10] Word [300000/400000]\n",
      "Iter [3/10] Word [0/400000]\n",
      "Iter [3/10] Word [100000/400000]\n",
      "Iter [3/10] Word [200000/400000]\n",
      "Iter [3/10] Word [300000/400000]\n",
      "Iter [4/10] Word [0/400000]\n",
      "Iter [4/10] Word [100000/400000]\n",
      "Iter [4/10] Word [200000/400000]\n",
      "Iter [4/10] Word [300000/400000]\n",
      "Iter [5/10] Word [0/400000]\n",
      "Iter [5/10] Word [100000/400000]\n",
      "Iter [5/10] Word [200000/400000]\n",
      "Iter [5/10] Word [300000/400000]\n",
      "Iter [6/10] Word [0/400000]\n",
      "Iter [6/10] Word [100000/400000]\n",
      "Iter [6/10] Word [200000/400000]\n",
      "Iter [6/10] Word [300000/400000]\n",
      "Iter [7/10] Word [0/400000]\n",
      "Iter [7/10] Word [100000/400000]\n",
      "Iter [7/10] Word [200000/400000]\n",
      "Iter [7/10] Word [300000/400000]\n",
      "Iter [8/10] Word [0/400000]\n",
      "Iter [8/10] Word [100000/400000]\n",
      "Iter [8/10] Word [200000/400000]\n",
      "Iter [8/10] Word [300000/400000]\n",
      "Iter [9/10] Word [0/400000]\n",
      "Iter [9/10] Word [100000/400000]\n",
      "Iter [9/10] Word [200000/400000]\n",
      "Iter [9/10] Word [300000/400000]\n",
      "Writing to Txt [0/400000]\n",
      "Writing to Txt [100000/400000]\n",
      "Writing to Txt [200000/400000]\n",
      "Writing to Txt [300000/400000]\n",
      "Score=53.20\n"
     ]
    }
   ],
   "source": [
    "from lexsub_check import precision\n",
    "\n",
    "word2neigh_oneside = get_oneside_neighbor_ws(lexFiles)\n",
    "newWordVects_oneside = retrofit(senseVecs, word2neigh_oneside, beta=1.0, alpha=2.0)\n",
    "np.save(\"newWordVects_oneside_wn.npy\", newWordVects_oneside)\n",
    "with open(\"newVects_oneside_wn.txt\", \"w\") as f:\n",
    "    count = 0\n",
    "    vSize = len(newWordVects_oneside)\n",
    "    for word, emb in newWordVects_oneside.items():\n",
    "        if (count % 100000) == 0:\n",
    "            print(\"Writing to Txt [{}/{}]\".format(count, vSize))\n",
    "            \n",
    "        f.write(word)\n",
    "        for num in emb:\n",
    "            f.write(\" \" + str(num))\n",
    "        f.write(\"\\n\")\n",
    "        count += 1\n",
    "\n",
    "# create pymagnitude file\n",
    "import subprocess\n",
    "subprocess.call([\"python\", \"-m\", \"pymagnitude.converter\", \"-i\", \"newVects_oneside_wn.txt\", \"-o\", \"newVects_oneside_wn.magnitude\"])\n",
    "\n",
    "\n",
    "lexsub = LexSub('newVects_oneside_wn.magnitude')\n",
    "output = []\n",
    "with open(os.path.join('../data','input','dev.txt')) as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        # fields is [index, sentence]\n",
    "        output.append(\" \".join(lexsub.substitutes(int(fields[0].strip()), fields[1].strip().split())))\n",
    "\n",
    "with open(os.path.join('../data','reference','dev.out'), 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "print(\"Score={:.2f}\".format(100*precision(ref_data, output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second method with context words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = []\n",
    "ctx = 6\n",
    "vectors = lexsub.wvecs\n",
    "\n",
    "lexsub.topn = 14\n",
    "with open(os.path.join('../data','input','dev.txt')) as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        dex = int(fields[0])\n",
    "        sentence = fields[1].split(\" \")\n",
    "        st = max(dex - ctx, 0)\n",
    "        end = min(dex + 1 + ctx, len(sentence))\n",
    "        cCtx = sentence[st:dex] + sentence[dex + 1:end]\n",
    "        \n",
    "        sims = lexsub.substitutes(int(fields[0].strip()), fields[1].strip().split())\n",
    "         \n",
    "        avgScores = []\n",
    "        balScores = []\n",
    "                \n",
    "        # Add method\n",
    "        for sim in sims:\n",
    "            tScore = vectors.similarity(sim, sentence[dex])\n",
    "            ctxScore = sum(vectors.similarity(sim, cCtx))\n",
    "            avgScore = (tScore + ctxScore) / (ctx * 2.0 + 1)\n",
    "            balScore = (len(cCtx) * tScore + ctxScore) / (len(cCtx))\n",
    "            balScores.append(balScore)\n",
    "            avgScores.append(avgScore)\n",
    "        \n",
    "        balScores = np.array(balScores)\n",
    "        avgScores = np.array(avgScores)\n",
    "        topWs = np.argsort(balScores)[-10:]\n",
    "        \n",
    "        res = []\n",
    "        for topW in topWs:\n",
    "            res.append(sims[topW])\n",
    "        \n",
    "        res = \" \".join(res)\n",
    "        output.append(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have a score of 49.79 after considering context words, this is much lower than our previous score without the post-process..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score=49.79\n"
     ]
    }
   ],
   "source": [
    "from lexsub_check import precision\n",
    "with open(os.path.join('../data','reference','dev.out'), 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "print(\"Score={:.2f}\".format(100*precision(ref_data, output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We also tried combining the results from the two result, but the score is always lower... It seems that the best score is 53.20 without using context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "output = []\n",
    "ctx = 6\n",
    "lexsub.topn=8\n",
    "lexsub_new = LexSub('newVects_oneside_wn.magnitude')\n",
    "vectors = lexsub_new.wvecs\n",
    "lexsub_new.topn = 14\n",
    "with open(os.path.join('../data','input','dev.txt')) as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        dex = int(fields[0])\n",
    "        sentence = fields[1].split(\" \")\n",
    "        st = max(dex - ctx, 0)\n",
    "        end = min(dex + 1 + ctx, len(sentence))\n",
    "        cCtx = sentence[st:dex] + sentence[dex + 1:end]\n",
    "        \n",
    "        cleanedCtx = []\n",
    "        for word in cCtx:\n",
    "            if word not in stopWords:\n",
    "                cleanedCtx.append(word)\n",
    "        \n",
    "        cCtx = cleanedCtx\n",
    "        \n",
    "        sims = lexsub_new.substitutes(int(fields[0].strip()), fields[1].strip().split())\n",
    "         \n",
    "        avgScores = []\n",
    "        balScores = []\n",
    "                \n",
    "        # Add method\n",
    "        for sim in sims:\n",
    "            tScore = vectors.similarity(sim, sentence[dex])\n",
    "            ctxScore = sum(vectors.similarity(sim, cCtx))\n",
    "            avgScore = (tScore + ctxScore) / (ctx * 2.0 + 1)\n",
    "            balScore = (len(cCtx) * tScore + ctxScore) / (len(cCtx))\n",
    "            balScores.append(balScore)\n",
    "            avgScores.append(avgScore)\n",
    "        \n",
    "        balScores = np.array(balScores)\n",
    "        avgScores = np.array(avgScores)\n",
    "        topWs = np.argsort(balScores)[-2:]\n",
    "\n",
    "        res = []\n",
    "        res_new = []\n",
    "        for topW in topWs:\n",
    "            res_new.append(sims[topW])\n",
    "        \n",
    "        #res_new = \" \".join(res_new)\n",
    "        res_old = lexsub.substitutes(int(fields[0].strip()), fields[1].strip().split())\n",
    "        res_shared = res_old + res_new\n",
    "        assert len(res_shared) == 10\n",
    "        res = \" \".join(res_shared)\n",
    "        output.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score=50.68\n"
     ]
    }
   ],
   "source": [
    "from lexsub_check import precision\n",
    "with open(os.path.join('../data','reference','dev.out'), 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "print(\"Score={:.2f}\".format(100*precision(ref_data, output)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
